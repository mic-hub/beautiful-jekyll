---
layout: post
title: Homework 8
subtitle: Web Scraping with Python
tags: [homework, digital humanities, beautiful soup, python]
comments: true
---

# Codecademy’s *Learn Python*

![Unit 9](/img/2019-05-18-Python-U9.png)
![Unit 10](/img/2019-05-18-Python-U10.png)


# Web Scraping with Python and Beautiful Soup

Disclaimer: I've *not* used Regexp to clean up the files; instead, I used the build-in get_text() from *Beautiful Soup*

> write a python script that will create clean copies of text from each issue of the “Dispatch” that you scraped before (make sure to keep the originals intact!).

``` python
import os
from bs4 import BeautifulSoup

pathToFolder = os.path.join(os.getcwd(), "data") # getcwd() -> get current working directory
listOfFiles = os.listdir(pathToFolder)

for fileName in listOfFiles:
	with open(os.path.join(pathToFolder, fileName), "r", encoding = "utf-8") as f: # f is just temporary variable
		dataOrig = BeautifulSoup(f.read(), 'lxml') # lxml is the superior parser, but it needs to be installed
	dataNew = dataOrig.get_text() # get_text() from BeautifulSoup automatically gets rid of any tags
	newFileName = fileName + "_modified.xml"
	with open(newFileName, "w", encoding = "utf-8") as f: # opens a new file in the current working directory
		f.write(dataNew) 
print("Finished!")
```

> write a python script that will create clean copies of articles (!) from all issues of the “Dispatch”. (again, make sure to keep the originals intact!).

``` python
import os
from bs4 import BeautifulSoup

sourceFolder = os.path.join(os.getcwd(), "data")
listOfFiles = os.listdir(sourceFolder)
targetFolder = os.path.join(os.getcwd(), "data_modified")

os.makedirs(targetFolder, exist_ok = True)
for fileName in listOfFiles:
	with open(os.path.join(sourceFolder, fileName), "r", encoding = "utf-8") as fr:
		dataOrig = BeautifulSoup(fr.read(), 'lxml')
	d = dataOrig.find_all("date", limit = 2)[1] # finds the first 2 date tags; only the 2nd list entry (count starts with 0) is useable
	issueDate = d.get("value") # gets the value from the attribute "value" which is the date
	allArticles = dataOrig.find_all("div3", {"type":"article"}) # generates list with all the articles
	for (i, a) in enumerate(allArticles[1:]): # i is the 0-based index; a is the current object
		singleArticle = a.get_text() # removes all the tags from the current list entry
		newFileName = issueDate + "_" + str(i)
		with open(os.path.join(targetFolder, newFileName), "w", encoding = "utf-8") as fw:
			fw.write(singleArticle)
print("Finished!")
```
